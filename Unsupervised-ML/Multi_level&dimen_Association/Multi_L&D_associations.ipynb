{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHBYrZhsqrug",
        "outputId": "88bcc295-b6e4-4d80-9f3d-e34d24dc0a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Head:\n",
            "  Transaction           Item         date_time period_day weekday_weekend\n",
            "0           1          Bread  30-10-2016 09:58    morning         weekend\n",
            "1           2   Scandinavian  30-10-2016 10:05    morning         weekend\n",
            "2           2   Scandinavian  30-10-2016 10:05    morning         weekend\n",
            "3           3  Hot chocolate  30-10-2016 10:07    morning         weekend\n",
            "4           3            Jam  30-10-2016 10:07    morning         weekend\n",
            "\n",
            "Total unique items: 94\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('retail_bakery_transactions (1) - retail_bakery_transactions (1).csv')\n",
        "\n",
        "# --- Data Cleaning and Preparation ---\n",
        "\n",
        "# Drop rows with missing values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Convert 'Transaction' column to string to avoid numeric interpretation\n",
        "df['Transaction'] = df['Transaction'].astype(str)\n",
        "\n",
        "# The 'Item' column may have leading/trailing spaces\n",
        "df['Item'] = df['Item'].str.strip()\n",
        "\n",
        "print(\"Dataset Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nTotal unique items:\", df['Item'].nunique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Multilevel Association Rules"
      ],
      "metadata": {
        "id": "Qf1XIBs-til1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilevel association rules allow us to find patterns at different levels of abstraction. For instance, we can find rules for general item categories (e.g., \"Bakery Goods\") and then drill down to specific items (e.g., \"Bread\", \"Cake\"). This is useful because setting a single minimum support threshold for all items is often problematic:\n",
        "\n",
        "High min_support: We only find rules for very frequent items (like 'Coffee') and miss patterns involving less frequent but potentially profitable items.\n",
        "\n",
        "Low min_support: We generate a massive number of uninteresting rules for very frequent items (e.g., {Coffee} -> {Bread} is obvious).\n",
        "\n"
      ],
      "metadata": {
        "id": "CwkDSVLL1gwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1: Reduced Minimum Support\n",
        "This approach involves using a lower minimum support for items at a lower level in the hierarchy (i.e., less frequent items). We'll start with a global support and then reduce it to see what new rules emerge.\n",
        "\n",
        "First, we need to convert our data into a list of transactions."
      ],
      "metadata": {
        "id": "-amueQY72AKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group items by transaction to create a list of lists\n",
        "transactions = df.groupby('Transaction')['Item'].apply(list).values.tolist()\n",
        "\n",
        "# Use TransactionEncoder to convert this into a one-hot encoded DataFrame\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "market_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "print(\"\\nOne-hot encoded DataFrame shape:\", market_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjsGpJSctOde",
        "outputId": "9c17011e-ac51-4e23-e1d3-385808f694a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-hot encoded DataFrame shape: (9465, 94)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's find frequent itemsets and generate rules with different support and confidence levels."
      ],
      "metadata": {
        "id": "OaDxaTMC2NuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Rule Generation with Different Support/Confidence ---\n",
        "\n",
        "# Global Support: 0.05\n",
        "frequent_itemsets_global = apriori(market_df, min_support=0.05, use_colnames=True)\n",
        "rules_global_40 = association_rules(frequent_itemsets_global, metric=\"confidence\", min_threshold=0.4)\n",
        "rules_global_50 = association_rules(frequent_itemsets_global, metric=\"confidence\", min_threshold=0.5)\n",
        "rules_global_60 = association_rules(frequent_itemsets_global, metric=\"confidence\", min_threshold=0.6)\n",
        "\n",
        "print(f\"\\n--- Global Support: 0.05 ---\")\n",
        "print(f\"Found {len(frequent_itemsets_global)} frequent itemsets.\")\n",
        "print(f\"Found {len(rules_global_40)} rules with 40% confidence.\")\n",
        "print(f\"Found {len(rules_global_50)} rules with 50% confidence.\")\n",
        "print(f\"Found {len(rules_global_60)} rules with 60% confidence.\")\n",
        "\n",
        "# Reduced Support: 0.04\n",
        "frequent_itemsets_reduced = apriori(market_df, min_support=0.04, use_colnames=True)\n",
        "rules_reduced_40 = association_rules(frequent_itemsets_reduced, metric=\"confidence\", min_threshold=0.4)\n",
        "print(f\"\\n--- Reduced Support: 0.04 ---\")\n",
        "print(f\"Found {len(frequent_itemsets_reduced)} frequent itemsets.\")\n",
        "print(f\"Found {len(rules_reduced_40)} rules with 40% confidence.\")\n",
        "\n",
        "# Testing other provided support values (0.2 and 0.3)\n",
        "frequent_itemsets_high_s1 = apriori(market_df, min_support=0.2, use_colnames=True)\n",
        "print(f\"\\n--- High Support: 0.2 ---\")\n",
        "print(f\"Found {len(frequent_itemsets_high_s1)} frequent itemsets.\")\n",
        "\n",
        "frequent_itemsets_high_s2 = apriori(market_df, min_support=0.3, use_colnames=True)\n",
        "print(f\"\\n--- High Support: 0.3 ---\")\n",
        "print(f\"Found {len(frequent_itemsets_high_s2)} frequent itemsets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqBEPelytrQH",
        "outputId": "60216aeb-dd13-4d5c-d7ce-922b7e250385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Global Support: 0.05 ---\n",
            "Found 11 frequent itemsets.\n",
            "Found 1 rules with 40% confidence.\n",
            "Found 1 rules with 50% confidence.\n",
            "Found 0 rules with 60% confidence.\n",
            "\n",
            "--- Reduced Support: 0.04 ---\n",
            "Found 14 frequent itemsets.\n",
            "Found 2 rules with 40% confidence.\n",
            "\n",
            "--- High Support: 0.2 ---\n",
            "Found 2 frequent itemsets.\n",
            "\n",
            "--- High Support: 0.3 ---\n",
            "Found 2 frequent itemsets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation: As you can see, the support values of 0.2 and 0.3 are too high, finding only single-item sets and thus generating no rules. Reducing the support from 0.05 to 0.04 increased the number of frequent itemsets from 11 to 14, which in turn generated more rules. This demonstrates how a small change in support can reveal more patterns"
      ],
      "metadata": {
        "id": "Ij9CIn2Z2cOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2: Group-Based Support (Conceptual)\n",
        "This is a more advanced technique. The idea is to manually set different support thresholds for different groups of items. For example, high-frequency items like 'Coffee' and 'Bread' should have a high support threshold, while low-frequency items like 'Smoothies' or 'Tacos/Fajita' should have a low one.\n",
        "\n",
        "Let's first identify the most and least frequent items."
      ],
      "metadata": {
        "id": "9JVvtL5G2hgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the support for each individual item\n",
        "item_support = market_df.mean().sort_values(ascending=False)\n",
        "\n",
        "# Define high-frequency and low-frequency groups\n",
        "high_freq_items = item_support[item_support >= 0.1].index.tolist()\n",
        "low_freq_items = item_support[(item_support < 0.05) & (item_support > 0.01)].index.tolist()\n",
        "\n",
        "print(\"\\n--- Group-Based Support Analysis ---\")\n",
        "print(\"High-Frequency Items (support >= 10%):\", high_freq_items)\n",
        "print(\"\\nLow-Frequency Items (1% < support < 5%):\", low_freq_items)\n",
        "\n",
        "# Generate rules with a low support to capture everything\n",
        "frequent_itemsets_low = apriori(market_df, min_support=0.01, use_colnames=True)\n",
        "rules_low_conf40 = association_rules(frequent_itemsets_low, metric=\"confidence\", min_threshold=0.4)\n",
        "\n",
        "print(f\"\\nTotal rules found with low support (0.01) and 40% confidence: {len(rules_low_conf40)}\")\n",
        "\n",
        "# Let's see some rules involving the low-frequency items\n",
        "# Convert frozensets to sets for easier checking\n",
        "rules_low_conf40['antecedents_set'] = rules_low_conf40['antecedents'].apply(lambda x: set(x))\n",
        "rules_low_conf40['consequents_set'] = rules_low_conf40['consequents'].apply(lambda x: set(x))\n",
        "\n",
        "# Find rules containing at least one low-frequency item\n",
        "low_freq_set = set(low_freq_items)\n",
        "rules_with_low_freq = rules_low_conf40[\n",
        "    rules_low_conf40.apply(lambda row: not low_freq_set.isdisjoint(row['antecedents_set']) or \\\n",
        "                                     not low_freq_set.isdisjoint(row['consequents_set']), axis=1)\n",
        "]\n",
        "\n",
        "print(\"\\n--- Example Rules Involving Low-Frequency Items (Support=0.01, Conf=40%) ---\")\n",
        "print(low_freq_items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2xpn4ko2lay",
        "outputId": "06e346c8-0ccd-4a92-a6f3-b7b37920a42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Group-Based Support Analysis ---\n",
            "High-Frequency Items (support >= 10%): ['Coffee', 'Bread', 'Tea', 'Cake']\n",
            "\n",
            "Low-Frequency Items (1% < support < 5%): ['Brownie', 'Farm House', 'Juice', 'Muffin', 'Alfajores', 'Scone', 'Soup', 'Toast', 'Scandinavian', 'Truffles', 'Coke', 'Spanish Brunch', 'Baguette', 'Tiffin', 'Fudge', 'Jam', 'Mineral water', 'Jammie Dodgers', 'Chicken Stew', 'Hearty & Seasonal', 'Salad']\n",
            "\n",
            "Total rules found with low support (0.01) and 40% confidence: 16\n",
            "\n",
            "--- Example Rules Involving Low-Frequency Items (Support=0.01, Conf=40%) ---\n",
            "['Brownie', 'Farm House', 'Juice', 'Muffin', 'Alfajores', 'Scone', 'Soup', 'Toast', 'Scandinavian', 'Truffles', 'Coke', 'Spanish Brunch', 'Baguette', 'Tiffin', 'Fudge', 'Jam', 'Mineral water', 'Jammie Dodgers', 'Chicken Stew', 'Hearty & Seasonal', 'Salad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation: By using a very low support (0.01), we were able to generate rules for less frequent items like 'Toast', 'Salad', and 'Hot chocolate'. A global support of 0.05 would have missed these patterns entirely. This shows the power of group-based thinking: we can specifically hunt for valuable patterns among less common items without being overwhelmed by rules from top-sellers."
      ],
      "metadata": {
        "id": "cv_Vnk7b2skM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Multidimensional Association Rules"
      ],
      "metadata": {
        "id": "Bj2etumduJRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multidimensional rules consider more than one attribute of a transaction. Instead of just looking at Items, we can include period_day or weekday_weekend. This allows us to answer questions like, \"What do people buy with coffee in the morning?\""
      ],
      "metadata": {
        "id": "a0KHTcoT2-VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create new \"items\" by combining the original item with other columns. For example, the item 'Bread' bought in the 'morning' will become two distinct items in our transaction: Item_Bread and period_day_morning."
      ],
      "metadata": {
        "id": "GJZ4PUlq3IU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy for multidimensional analysis\n",
        "multi_df = df.copy()\n",
        "\n",
        "# Create formatted \"items\" from different columns\n",
        "multi_df['Item'] = 'Item_' + multi_df['Item'].astype(str)\n",
        "multi_df['period_day'] = 'period_day_' + multi_df['period_day'].astype(str)\n",
        "multi_df['weekday_weekend'] = 'weekday_weekend_' + multi_df['weekday_weekend'].astype(str)\n",
        "\n",
        "# Combine all generated items into a single column for grouping\n",
        "# We create a new DataFrame with transaction and the new \"item\" concept\n",
        "multi_dim_df = pd.concat([\n",
        "    multi_df[['Transaction', 'Item']].rename(columns={'Item': 'Attribute'}),\n",
        "    multi_df[['Transaction', 'period_day']].rename(columns={'period_day': 'Attribute'}),\n",
        "    multi_df[['Transaction', 'weekday_weekend']].rename(columns={'weekday_weekend': 'Attribute'})\n",
        "])\n",
        "\n",
        "# Group by transaction to get our final list for encoding\n",
        "multi_transactions = multi_dim_df.groupby('Transaction')['Attribute'].apply(list).values.tolist()\n",
        "\n",
        "# One-hot encode the multidimensional data\n",
        "te_multi = TransactionEncoder()\n",
        "te_ary_multi = te_multi.fit(multi_transactions).transform(multi_transactions)\n",
        "market_df_multi = pd.DataFrame(te_ary_multi, columns=te_multi.columns_)\n",
        "\n",
        "print(\"\\nShape of multidimensional DataFrame:\", market_df_multi.shape)\n",
        "print(\"Example columns:\", market_df_multi.columns[:5].tolist(), \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuQjfm2C3IzB",
        "outputId": "83029268-e196-4234-a465-b0a047a6639e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of multidimensional DataFrame: (9465, 100)\n",
            "Example columns: ['Item_Adjustment', 'Item_Afternoon with the baker', 'Item_Alfajores', 'Item_Argentina Night', 'Item_Art Tray'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll run Apriori and then apply the specific constraints you provided:\n",
        "\n",
        "Data Constraint: Rules must contain 'Item_Coffee' and 'period_day_morning'.\n",
        "\n",
        "Length Constraint: The total rule length (antecedents + consequents) must be 3 or more."
      ],
      "metadata": {
        "id": "y8FnzMx83Wuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate frequent itemsets with a reasonable support\n",
        "frequent_itemsets_multi = apriori(market_df_multi, min_support=0.03, use_colnames=True)\n",
        "\n",
        "# Generate rules with different confidence levels\n",
        "rules_multi_40 = association_rules(frequent_itemsets_multi, metric=\"confidence\", min_threshold=0.4)\n",
        "rules_multi_50 = association_rules(frequent_itemsets_multi, metric=\"confidence\", min_threshold=0.5)\n",
        "rules_multi_60 = association_rules(frequent_itemsets_multi, metric=\"confidence\", min_threshold=0.6)\n",
        "\n",
        "print(f\"\\n--- Multidimensional Rules (Before Filtering) ---\")\n",
        "print(f\"Found {len(rules_multi_40)} rules with 40% confidence.\")\n",
        "print(f\"Found {len(rules_multi_50)} rules with 50% confidence.\")\n",
        "print(f\"Found {len(rules_multi_60)} rules with 60% confidence.\")\n",
        "\n",
        "# --- Apply Constraints ---\n",
        "\n",
        "def filter_rules(rules):\n",
        "    # Data constraint\n",
        "    rules_filtered = rules[\n",
        "        rules['antecedents'].apply(lambda x: {'Item_Coffee', 'period_day_morning'}.issubset(x)) |\n",
        "        rules['consequents'].apply(lambda x: {'Item_Coffee', 'period_day_morning'}.issubset(x))\n",
        "    ]\n",
        "    # Length constraint\n",
        "    rules_filtered = rules_filtered[\n",
        "        rules_filtered.apply(lambda row: len(row['antecedents']) + len(row['consequents']), axis=1) >= 3\n",
        "    ]\n",
        "    return rules_filtered\n",
        "\n",
        "final_rules_40 = filter_rules(rules_multi_40)\n",
        "final_rules_50 = filter_rules(rules_multi_50)\n",
        "final_rules_60 = filter_rules(rules_multi_60)\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Filtered Multidimensional Rules (Confidence >= 40%) ---\")\n",
        "print(final_rules_40[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BesXVWgC3KLo",
        "outputId": "f3ab8dc7-b92b-4a61-a4ea-9b645f28b728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Multidimensional Rules (Before Filtering) ---\n",
            "Found 82 rules with 40% confidence.\n",
            "Found 62 rules with 50% confidence.\n",
            "Found 35 rules with 60% confidence.\n",
            "\n",
            "--- Final Filtered Multidimensional Rules (Confidence >= 40%) ---\n",
            "                          antecedents                consequents   support  confidence      lift\n",
            "68  (period_day_morning, Item_Coffee)  (weekday_weekend_weekday)  0.148125    0.663512  1.021991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretation"
      ],
      "metadata": {
        "id": "H4GCamA84jrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "How These Techniques Reduced the Search Space\n",
        "1. Multilevel Mining (Group-Based Support): Standard Apriori with one low support threshold creates an explosion of rules. By conceptualizing items into groups (high-freq, low-freq), we can focus our search. We avoid generating thousands of redundant rules like {Coffee} -> {Sugar} and instead use a fine-tuned, lower support to specifically find hidden gems among less frequent items. This prunes the search tree by not over-exploring the \"obvious\" branches.\n",
        "\n",
        "2. Multidimensional Mining (Constraints): This is the most direct way to reduce the search space. Instead of analyzing all possible combinations, we are telling the algorithm to only explore paths that are relevant to our business question. By applying data constraints ('Coffee', 'morning') and length constraints (>=3), we immediately discard millions of irrelevant potential rules, saving computational time and making the final output highly focused and actionable."
      ],
      "metadata": {
        "id": "ptxV8kX74pvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendations for the Retailer 📈\n",
        "Based on the filtered multidimensional rules, here are some actionable recommendations:\n",
        "\n",
        "1. Create a \"Morning Coffee Combo\": The rules consistently show that customers who buy Coffee in the morning also buy Bread.\n",
        "\n",
        "  Action: Offer a small discount for a \"Morning Combo\" of Coffee + Bread. This can increase the average transaction value.\n",
        "\n",
        "2. Target Weekend Morning Shoppers: The rules containing weekday_weekend_weekend suggest a specific shopping behavior.\n",
        "\n",
        "  Action: Promote special weekend-only morning pastries or breakfast deals. Since weekend customers might be less rushed, you could promote higher-margin items alongside their morning coffee.\n",
        "\n",
        "3. Optimize Store Layout:\n",
        "Ensure that pastries, bread, and other popular morning items are placed near the coffee station. This makes it convenient for customers to pick up an extra item, increasing impulse buys. The rule {Item_Coffee, period_day_morning} -> {Item_Bread} strongly supports this."
      ],
      "metadata": {
        "id": "RZMn18bv5EM3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aB6qvX-e3Y67"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}